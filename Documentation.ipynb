{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kApQMqB6z7Ib",
        "colab_type": "text"
      },
      "source": [
        "**The wake-sleep algorithm for unsupervised neural networks** <br>\n",
        "<br>\n",
        "Geoffrey E Hinton, Peter Dayan, Brendan J Frey, Radford M Neal![alt text](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r66Kwsh30EPQ",
        "colab_type": "text"
      },
      "source": [
        "**Introduction** <br>\n",
        "As Artificial Neural Networks are based on simplified neuronal structures in the brain often not only the lower level idea of information processing via neurons is strongly influenced by natural brain processes but also the higher level model structure. In April 1995 the wake-sleep algorithm for unsupervised neural networks was published. Its main idea is inspired by the natural wake and sleep periods with respect to their impact on learning. However, what makes this algorithm especially interesting is the fact that it is based on unsupervised learning whereas most neuronal networks are dealing with supervised learning tasks.\n",
        "<br>\n",
        "<br>\n",
        "In the following we want to rebuild and explain the different ideas behind the algorithm and evaluate its performance via the common MNIST dataset. In order to do so we start with the smallest unit, the neurons and end with the overall network structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7FKLhcv0HM8",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>TODO: Introduction</font> <br>\n",
        "**General Idea**\n",
        "\n",
        "- concrete inspiration (wake - sleep)\n",
        "- no teacher needed, instead the description length of the input is compared to description length of hidden representation\n",
        "- Bottom - up: Create hidden representation\n",
        "- Top - down: Recreate representation from hidden representation and compare to input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhBBPCwm0NZy",
        "colab_type": "text"
      },
      "source": [
        "**Binary Stochastic Neurons**<br>\n",
        "\n",
        "Eventhough the explicit type of the stochastic neurons is not decisive for the network structure for simplification binary stochastic neurons are used in this implementation. Those units have states of either 0 or 1 depending on the previously calculated probability:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSIXjMLA0RGr",
        "colab_type": "text"
      },
      "source": [
        "(1)$$Prob(s_v = 1) = \\frac{1}{1 + exp(-b_v - \\sum_u s_u w_{uv})}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-C5PYy0QWs",
        "colab_type": "text"
      },
      "source": [
        "In the implementation each neuron contains information about its input weights, its bias, the probability and its state (either 0 or 1). As the amount of weights depends on the size of the input this variable is unavoidable for the initialization. <br>\n",
        "<font color='red'>TODO: Initialization + comments about initialization</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-veVY-90VEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Stochastic_Neuron:\n",
        "    def __init__(self, input_size):      \n",
        "        self.input_weights = np.random.normal(loc=0,scale=0.5,size=input_size)\n",
        "        self.bias = 0 \n",
        "        self.state = 0      \n",
        "        self.prob = 0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G4wfAVE0cwp",
        "colab_type": "text"
      },
      "source": [
        "Formula (1) is the activation function for each neuron and results in a probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CdE6ARQ0aI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def activation_probability(self, inputs):\n",
        "        self.prob = tf.math.sigmoid(- self.bias - np.sum(self.input_weights * inputs))\n",
        "        return self.prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAmxYcKE0fmY",
        "colab_type": "text"
      },
      "source": [
        "However, as the neuron is binary, this probability still has to be converted into 0 or 1. This is achived via the binomial distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i290SSvE0eno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def __call__(self, inputs):\n",
        "        self.state = np.random.binomial(1, self.activation_probability(inputs))\n",
        "        return self.state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzluUbL30n29",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>Include examples?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6l1ir240qcp",
        "colab_type": "text"
      },
      "source": [
        "**Layer**\n",
        "<br>\n",
        "<br>\n",
        "For the next level of the network the neurons are combined in layers as usual. Therefore each layers consist of a list of neurons each initialized with the input size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7KT3hnM0kPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer(input_size, layer_size):\n",
        "    return [Stochastic_Neuron(input_size) for i in range(layer_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en2VfUD90tih",
        "colab_type": "text"
      },
      "source": [
        "Up to this point the network modules are relatively similar to those of the standard ANN. In the network class they are put together according to the idea of wake and sleep phases:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylkaekNj0tRF",
        "colab_type": "text"
      },
      "source": [
        "1. Build up the whole network consisting of multiple layers with different amounts of neurons respectively. The input size of each layer depends on the layer_size of the previous layer or the input size in the case of the first layer, whereas the layer sizes are defined in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tolBKUiD0s2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network:\n",
        "\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layers = [layer(layer_sizes[idx], layer_sizes[idx+1]) for idx in range(len(layer_sizes)-1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dhAXDSq04l2",
        "colab_type": "text"
      },
      "source": [
        "2. As the information flow stays the same in both the recognition and the generative model, it is sufficient to work on one Network class. Starting with the input, the information is travelling via the neurons through the whole network. As we need the states of all neurons for learning, they are stored in a list. Eventhough the calculations are based on the binary output also the probabolities are stored for the loss calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKNbXkjL0sVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def infer(self, inputs):\n",
        "        states = [inputs]\n",
        "        activations = [] # only for loss!\n",
        "        for layer in self.layers:\n",
        "            states.append([neuron(states[-1]) for neuron in layer])\n",
        "            activations.append([neuron.prob for neuron in layer]) # only for loss!\n",
        "\n",
        "        self.network_probs = activations[:-1] \n",
        "\n",
        "     \n",
        "        return states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pY5mS3216ol",
        "colab_type": "text"
      },
      "source": [
        "3. The learning function is the main part of the wake-sleep algorithm. The updates are not based on the information of a single model, but rather depend on the states of the previous model and the probabilities of the current one. With the update formula (2) for the generative weights:  $$ \\Delta w_{kj} = \\epsilon s^\\alpha_k(s^\\alpha_j - q^\\alpha_j)$$\n",
        "<br>\n",
        "and the update formula (3) for the recognition weights:\n",
        "<br>\n",
        "<br>\n",
        "$$ \\Delta w_{jk} = \\epsilon s^\\gamma_j(s^\\gamma_k - q^\\gamma_k)$$\n",
        "<br>\n",
        "the network is updated. The learning rate $\\epsilon$ stays the same in both phases. The states $s$ are infered from the network that is not updated at the moment where the first states are the prior output states and the second states refer to the states of the current layer. The probability $p$ / $q$ however belongs to the currently updating network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKqiwCok2AXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def learning(self, other_network_states, epsilon):\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "\n",
        "            layer_states = other_network_states[idx+1] # in der klammer \n",
        "            prior_states = other_network_states[idx] # auserhalb der klammer\n",
        "\n",
        "            for idx, neuron in enumerate(layer):\n",
        "                neuron.input_weights += np.array([epsilon * prior * (layer_states[idx] - neuron.prob) for prior in prior_states]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiJh8z_CAFzk",
        "colab_type": "text"
      },
      "source": [
        "**Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpYocdKq2CLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the loss of the network\n",
        "def calculate_loss(rec_probs, gen_probs):\n",
        "  loss = 0\n",
        "  for l_rec, l_gen in zip(rec_probs, gen_probs[::-1]):\n",
        "        l_rec = np.array(l_rec)\n",
        "        l_gen = np.array(l_gen)\n",
        "        loss += np.sum(l_gen * np.log(l_gen / l_rec) + (1 - l_gen) * np.log((1 - l_gen)/(1 - l_rec)))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ujSfyFoAIJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}